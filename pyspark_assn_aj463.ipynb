{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0f159b2-d2e5-4046-972f-e840b01580ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Spark session created successfully.\n",
      "================================================================================\n",
      "COVID-19 DATA PROCESSING PIPELINE\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window\n",
    "import time\n",
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "\n",
    "spark = SparkSession.builder.appName(\"MusicListeningDataPipeline\").getOrCreate()\n",
    "\n",
    "\n",
    "print(\"✅ Spark session created successfully.\")\n",
    "\n",
    "# Initialize Spark Session with optimized configurations\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"COVID-19 Data Pipeline\")\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"COVID-19 DATA PROCESSING PIPELINE\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "021e6bd8-f16d-4fce-b01d-f404bba0f393",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows: 1,227,256\n",
      "⏱️  Time to load data: 1.78491 seconds\n",
      "root\n",
      " |-- date: string (nullable = true)\n",
      " |-- county: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- fips: string (nullable = true)\n",
      " |-- cases: string (nullable = true)\n",
      " |-- deaths: string (nullable = true)\n",
      "\n",
      "+----------+---------+----------+-----+-----+------+\n",
      "|      date|   county|     state| fips|cases|deaths|\n",
      "+----------+---------+----------+-----+-----+------+\n",
      "|2020-01-21|Snohomish|Washington|53061|    1|     0|\n",
      "|2020-01-22|Snohomish|Washington|53061|    1|     0|\n",
      "|2020-01-23|Snohomish|Washington|53061|    1|     0|\n",
      "|2020-01-24|     Cook|  Illinois|17031|    1|     0|\n",
      "|2020-01-24|Snohomish|Washington|53061|    1|     0|\n",
      "+----------+---------+----------+-----+-----+------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# LOAD DATA WITH DEFAULT PARTITIONING\n",
    "# ============================================================================\n",
    "\n",
    "# Start timing\n",
    "start_time = time.time()\n",
    "\n",
    "# Read the dataset\n",
    "covid_df = spark.read.csv(\n",
    "    \"/databricks-datasets/COVID/covid-19-data/\", header=True, inferSchema=True\n",
    ")\n",
    "\n",
    "# Trigger action to actually load the data (transformations are lazy!)\n",
    "row_count = covid_df.count()\n",
    "\n",
    "# Calculate elapsed time\n",
    "load_time = time.time() - start_time\n",
    "\n",
    "# Check the data\n",
    "print(f\"Total rows: {covid_df.count():,}\")\n",
    "print(f\"⏱️  Time to load data: {load_time:.5f} seconds\")\n",
    "covid_df.printSchema()\n",
    "covid_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce32be88-9538-4bf0-a91f-176a1871e017",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows: 1,227,256\n",
      "⏱️  Time to load data: 2.20983 seconds\n",
      "root\n",
      " |-- date: string (nullable = true)\n",
      " |-- county: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- fips: string (nullable = true)\n",
      " |-- cases: string (nullable = true)\n",
      " |-- deaths: string (nullable = true)\n",
      "\n",
      "+----------+-----------+----------+-----+-----+------+\n",
      "|      date|     county|     state| fips|cases|deaths|\n",
      "+----------+-----------+----------+-----+-----+------+\n",
      "|2020-01-25|     Orange|California|06059|    1|     0|\n",
      "|2020-01-27|Los Angeles|California|06037|    1|     0|\n",
      "|2020-01-28|  Snohomish|Washington|53061|    1|     0|\n",
      "|2020-01-30|   Maricopa|   Arizona|04013|    1|     0|\n",
      "|2020-01-31|  Snohomish|Washington|53061|    1|     0|\n",
      "+----------+-----------+----------+-----+-----+------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# LOAD DATA WITH SPECIFIED NUMBER OF PARTITIONS\n",
    "# ============================================================================\n",
    "# Start timing\n",
    "start_time = time.time()\n",
    "\n",
    "# Read the dataset with specified number of partitions\n",
    "covid_df = spark.read.csv(\n",
    "    \"/databricks-datasets/COVID/covid-19-data/\", header=True, inferSchema=True\n",
    ").repartition(\n",
    "    8\n",
    ")  # Define number of partitions (adjust based on cluster size)\n",
    "\n",
    "# Trigger action to actually load the data (transformations are lazy!)\n",
    "row_count = covid_df.count()\n",
    "\n",
    "# Calculate elapsed time\n",
    "load_time = time.time() - start_time\n",
    "\n",
    "# Check the data\n",
    "print(f\"Total rows: {row_count:,}\")\n",
    "# print(f\"Number of partitions: {covid_df.rdd.getNumPartitions()}\")\n",
    "print(f\"⏱️  Time to load data: {load_time:.5f} seconds\")\n",
    "covid_df.printSchema()\n",
    "covid_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34678512-912c-4007-ba48-eb78da6a6430",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total size: 2,567,706,254 bytes\n",
      "Total size: 2448.76 MB\n",
      "Total size: 2.39 GB\n"
     ]
    }
   ],
   "source": [
    "# Checkthe size of the dataset\n",
    "def get_directory_size(path):\n",
    "    total_size = 0\n",
    "    try:\n",
    "        files = dbutils.fs.ls(path)\n",
    "        for file in files:\n",
    "            if file.isDir():\n",
    "                total_size += get_directory_size(file.path)\n",
    "            else:\n",
    "                total_size += file.size\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "    return total_size\n",
    "\n",
    "\n",
    "path = \"/databricks-datasets/COVID/covid-19-data/\"\n",
    "size_bytes = get_directory_size(path)\n",
    "size_mb = size_bytes / (1024 * 1024)\n",
    "size_gb = size_bytes / (1024 * 1024 * 1024)\n",
    "print(f\"Total size: {size_bytes:,} bytes\")\n",
    "print(f\"Total size: {size_mb:.2f} MB\")\n",
    "print(f\"Total size: {size_gb:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "107e3336-c367-4c9e-8770-5ed618223e31",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "CREATING REFERENCE DATA\n",
      "================================================================================\n",
      "State population reference data loaded: 6 states\n",
      "+----------+----------+---------+\n",
      "|     state|population|   region|\n",
      "+----------+----------+---------+\n",
      "|California|  39538223|     West|\n",
      "|  New York|  20201249|Northeast|\n",
      "|     Texas|  29145505|    South|\n",
      "|   Florida|  21538187|    South|\n",
      "|Washington|   7705281|     West|\n",
      "|  Illinois|  12812508|  Midwest|\n",
      "+----------+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CREATE REFERENCE DATA FOR JOIN\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"CREATING REFERENCE DATA\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Create a second dataset for join operation (simulate external data)\n",
    "# This represents state population data\n",
    "state_population = spark.createDataFrame(\n",
    "    [\n",
    "        (\"California\", 39538223, \"West\"),\n",
    "        (\"New York\", 20201249, \"Northeast\"),\n",
    "        (\"Texas\", 29145505, \"South\"),\n",
    "        (\"Florida\", 21538187, \"South\"),\n",
    "        (\"Washington\", 7705281, \"West\"),\n",
    "        (\"Illinois\", 12812508, \"Midwest\"),\n",
    "    ],\n",
    "    [\"state\", \"population\", \"region\"],\n",
    ")\n",
    "\n",
    "print(f\"State population reference data loaded: {state_population.count()} states\")\n",
    "state_population.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b4d1f274-8538-42bd-9cbd-d698b9aa0189",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "VERSION 1: EAGER/INEFFICIENT APPROACH (Bad Practices)\n",
      "================================================================================\n",
      "Problems:\n",
      "  ❌ GroupBy BEFORE filtering (processes all 1.2M rows)\n",
      "  ❌ Join on full dataset BEFORE filtering\n",
      "  ❌ Column transformations on large dataset\n",
      "  ❌ Multiple shuffles due to poor ordering\n",
      "  ❌ No early data reduction\n",
      "\n",
      "[Step 1] Performing groupBy on ENTIRE dataset...\n",
      "  Time: 0.00s | Rows: 1,133,113\n",
      "\n",
      "[Step 2] Adding column transformations on full dataset...\n",
      "  Time: 0.00s\n",
      "\n",
      "[Step 3] Joining with population data on full dataset...\n",
      "  Time: 0.00s | Rows: 1,133,113\n",
      "\n",
      "[Step 4] Finally filtering (after all expensive operations)...\n",
      "  Time: 0.00s | Rows: 131,570\n",
      "\n",
      "[Step 5] Second filter (separate operation)...\n",
      "  Time: 0.00s | Rows: 115,697\n",
      "\n",
      "[Step 6] Final aggregation (another shuffle)...\n",
      "  Time: 0.81s | Final rows: 52\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "EAGER APPROACH SUMMARY:\n",
      "  Total execution time: 4.29 seconds\n",
      "  Number of shuffles: ~4-5 (groupBy, join, filters, final groupBy)\n",
      "  Data processed: Full dataset multiple times\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Sample results (Eager approach):\n",
      "+----------+------+----+-----+-------------+--------------+------------------+-----------------+\n",
      "|state     |region|year|month|monthly_cases|monthly_deaths|avg_mortality_rate|counties_reported|\n",
      "+----------+------+----+-----+-------------+--------------+------------------+-----------------+\n",
      "|California|West  |2020|2    |44.0         |0.0           |0.0               |4                |\n",
      "|California|West  |2020|3    |53893.0      |1075.0        |1.953142131176942 |468              |\n",
      "|California|West  |2020|4    |864363.0     |30166.0       |3.2309658387784115|1234             |\n",
      "|California|West  |2020|5    |2475042.0    |99347.0       |3.0996459026429473|1411             |\n",
      "|California|West  |2020|6    |4904141.0    |155680.0      |2.24890510173643  |1502             |\n",
      "|California|West  |2020|7    |1.140477E7   |232621.0      |1.2139232466490364|1671             |\n",
      "|California|West  |2020|8    |1.9168191E7  |349458.0      |1.3111670072947814|1683             |\n",
      "|California|West  |2020|9    |2.3157578E7  |438311.0      |1.4751735752790123|1680             |\n",
      "|California|West  |2020|10   |2.7163707E7  |522654.0      |1.5523338388611116|1736             |\n",
      "|California|West  |2020|11   |3.1809052E7  |551791.0      |1.3805990606541387|1720             |\n",
      "+----------+------+----+-----+-------------+--------------+------------------+-----------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "EXECUTION PLAN (Eager - Notice multiple stages and shuffles):\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- == Initial Plan ==\n",
      "   ColumnarToRow\n",
      "   +- PhotonResultStage\n",
      "      +- PhotonSort [state#14466 ASC NULLS FIRST, year#15344 ASC NULLS FIRST, month#15346 ASC NULLS FIRST]\n",
      "         +- PhotonShuffleExchangeSource\n",
      "            +- PhotonShuffleMapStage ENSURE_REQUIREMENTS, [id=#19072]\n",
      "               +- PhotonShuffleExchangeSink rangepartitioning(state#14466 ASC NULLS FIRST, year#15344 ASC NULLS FIRST, month#15346 ASC NULLS FIRST, 1024)\n",
      "                  +- PhotonProject [state#14466, region#15355, year#15344, month#15346, sum(total_cases)#15357 AS monthly_cases#15349, sum(total_deaths)#15358 AS monthly_deaths#15350, avg(mortality_rate)#15359 AS avg_mortality_rate#15351, count(1)#15356L AS counties_reported#15352L]\n",
      "                     +- PhotonGroupingAgg(keys=[state#14466, region#15355, year#15344, month#15346], functions=[finalmerge_sum(merge sum#15361) AS sum(total_cases)#15357, finalmerge_sum(merge sum#15363) AS sum(total_deaths)#15358, finalmerge_avg(merge sum#15366, count#15367L) AS avg(mortality_rate)#15359, finalmerge_count(merge count#15369L) AS count(1)#15356L])\n",
      "                        +- PhotonShuffleExchangeSource\n",
      "                           +- PhotonShuffleMapStage ENSURE_REQUIREMENTS, [id=#19064]\n",
      "                              +- PhotonShuffleExchangeSink hashpartitioning(state#14466, region#15355, year#15344, month#15346, 1024)\n",
      "                                 +- PhotonGroupingAgg(keys=[state#14466, region#15355, year#15344, month#15346], functions=[merge_sum(merge sum#15361) AS sum#15361, merge_sum(merge sum#15363) AS sum#15363, merge_avg(merge sum#15366, count#15367L) AS (sum#15366, count#15367L), merge_count(merge count#15369L) AS count#15369L])\n",
      "                                    +- PhotonProject [state#14466, region#15355, year#15344, month#15346, sum#15361, sum#15363, sum#15366, count#15367L, count#15369L]\n",
      "                                       +- PhotonBroadcastHashJoin [state#14466], [state#15353], LeftOuter, BuildRight, false, true\n",
      "                                          :- PhotonGroupingAgg(keys=[state#14466, year#15344, month#15346], functions=[partial_sum(total_cases#15337) AS sum#15361, partial_sum(total_deaths#15338) AS sum#15363, partial_avg(mortality_rate#15342) AS (sum#15366, count#15367L), partial_count(1) AS count#15369L])\n",
      "                                          :  +- PhotonProject [state#14466, total_cases#15337, total_deaths#15338, CASE WHEN (total_cases#15337 > 0.0) THEN ((total_deaths#15338 / total_cases#15337) * 100.0) ELSE 0.0 END AS mortality_rate#15342, year(cast(date#14464 as date)) AS year#15344, month(cast(date#14464 as date)) AS month#15346]\n",
      "                                          :     +- PhotonFilter (isnotnull(total_cases#15337) AND (total_cases#15337 >= 10.0))\n",
      "                                          :        +- PhotonGroupingAgg(keys=[state#14466, county#14465, date#14464], functions=[finalmerge_sum(merge sum#15373) AS sum(cases)#15339, finalmerge_sum(merge sum#15375) AS sum(deaths)#15340])\n",
      "                                          :           +- PhotonShuffleExchangeSource\n",
      "                                          :              +- PhotonShuffleMapStage ENSURE_REQUIREMENTS, [id=#19039]\n",
      "                                          :                 +- PhotonShuffleExchangeSink hashpartitioning(state#14466, county#14465, date#14464, 1024)\n",
      "                                          :                    +- PhotonGroupingAgg(keys=[state#14466, county#14465, date#14464], functions=[partial_sum(cast(cases#14468 as double)) AS sum#15373, partial_sum(cast(deaths#14469 as double)) AS sum#15375])\n",
      "                                          :                       +- PhotonFilter ((isnotnull(date#14464) AND (year(cast(date#14464 as date)) = 2020)) AND state#14466 IN (California,New York,Texas,Florida,Washington))\n",
      "                                          :                          +- PhotonRowToColumnar\n",
      "                                          :                             +- FileScan csv [date#14464,county#14465,state#14466,cases#14468,deaths#14469] Batched: false, DataFilters: [isnotnull(date#14464), (year(cast(date#14464 as date)) = 2020), state#14466 IN (California,New Y..., Format: CSV, Location: InMemoryFileIndex(1 paths)[dbfs:/databricks-datasets/COVID/covid-19-data], PartitionFilters: [], PushedFilters: [IsNotNull(date), In(state, [California,Florida,New York,Texas,Washington])], ReadSchema: struct<date:string,county:string,state:string,cases:string,deaths:string>\n",
      "                                          +- PhotonShuffleExchangeSource\n",
      "                                             +- PhotonShuffleMapStage EXECUTOR_BROADCAST, [id=#19055]\n",
      "                                                +- PhotonShuffleExchangeSink SinglePartition\n",
      "                                                   +- PhotonFilter (isnotnull(state#15353) AND state#15353 IN (California,New York,Texas,Florida,Washington))\n",
      "                                                      +- PhotonRowToColumnar\n",
      "                                                         +- LocalTableScan [state#15353, region#15355]\n",
      "\n",
      "\n",
      "== Photon Explanation ==\n",
      "The query is fully supported by Photon.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# VERSION 1: EAGER/INEFFICIENT APPROACH ❌\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"VERSION 1: EAGER/INEFFICIENT APPROACH (Bad Practices)\")\n",
    "print(\"=\" * 80)\n",
    "print(\"Problems:\")\n",
    "print(\"  ❌ GroupBy BEFORE filtering (processes all 1.2M rows)\")\n",
    "print(\"  ❌ Join on full dataset BEFORE filtering\")\n",
    "print(\"  ❌ Column transformations on large dataset\")\n",
    "print(\"  ❌ Multiple shuffles due to poor ordering\")\n",
    "print(\"  ❌ No early data reduction\")\n",
    "\n",
    "eager_start = time.time()\n",
    "\n",
    "# Step 1: GroupBy FIRST (BAD - processes all data)\n",
    "print(\"\\n[Step 1] Performing groupBy on ENTIRE dataset...\")\n",
    "step1_start = time.time()\n",
    "eager_grouped = covid_df.groupBy(\"state\", \"county\", \"date\").agg(\n",
    "    sum(\"cases\").alias(\"total_cases\"), sum(\"deaths\").alias(\"total_deaths\")\n",
    ")\n",
    "step1_time = time.time() - step1_start\n",
    "print(f\"  Time: {step1_time:.2f}s | Rows: {eager_grouped.count():,}\")\n",
    "\n",
    "# Step 2: Add column transformations BEFORE filtering (BAD - transforms all rows)\n",
    "print(\"\\n[Step 2] Adding column transformations on full dataset...\")\n",
    "step2_start = time.time()\n",
    "eager_transformed = (\n",
    "    eager_grouped.withColumn(\n",
    "        \"mortality_rate\",\n",
    "        when(\n",
    "            col(\"total_cases\") > 0, (col(\"total_deaths\") / col(\"total_cases\")) * 100\n",
    "        ).otherwise(0),\n",
    "    )\n",
    "    .withColumn(\"year\", year(col(\"date\")))\n",
    "    .withColumn(\"month\", month(col(\"date\")))\n",
    "    .withColumn(\n",
    "        \"case_category\",\n",
    "        when(col(\"total_cases\") < 100, \"Low\")\n",
    "        .when(col(\"total_cases\") < 1000, \"Medium\")\n",
    "        .otherwise(\"High\"),\n",
    "    )\n",
    ")\n",
    "step2_time = time.time() - step2_start\n",
    "print(f\"  Time: {step2_time:.2f}s\")\n",
    "\n",
    "# Step 3: Join BEFORE filtering (BAD - joins large dataset)\n",
    "print(\"\\n[Step 3] Joining with population data on full dataset...\")\n",
    "step3_start = time.time()\n",
    "eager_joined = eager_transformed.join(state_population, on=\"state\", how=\"left\")\n",
    "step3_time = time.time() - step3_start\n",
    "print(f\"  Time: {step3_time:.2f}s | Rows: {eager_joined.count():,}\")\n",
    "\n",
    "# Step 4: Filter LAST (BAD - after all expensive operations)\n",
    "print(\"\\n[Step 4] Finally filtering (after all expensive operations)...\")\n",
    "step4_start = time.time()\n",
    "eager_filtered = eager_joined.filter(\n",
    "    (col(\"year\") == 2020)\n",
    "    & (col(\"state\").isin(\"California\", \"New York\", \"Texas\", \"Florida\", \"Washington\"))\n",
    ")\n",
    "step4_time = time.time() - step4_start\n",
    "print(f\"  Time: {step4_time:.2f}s | Rows: {eager_filtered.count():,}\")\n",
    "\n",
    "# Step 5: Another filter (BAD - separate operation causes another pass)\n",
    "print(\"\\n[Step 5] Second filter (separate operation)...\")\n",
    "step5_start = time.time()\n",
    "eager_final = eager_filtered.filter(col(\"total_cases\") >= 10)\n",
    "step5_time = time.time() - step5_start\n",
    "print(f\"  Time: {step5_time:.2f}s | Rows: {eager_final.count():,}\")\n",
    "\n",
    "# Step 6: Final aggregation with another groupBy (causes another shuffle)\n",
    "print(\"\\n[Step 6] Final aggregation (another shuffle)...\")\n",
    "step6_start = time.time()\n",
    "eager_result = (\n",
    "    eager_final.groupBy(\"state\", \"region\", \"year\", \"month\")\n",
    "    .agg(\n",
    "        sum(\"total_cases\").alias(\"monthly_cases\"),\n",
    "        sum(\"total_deaths\").alias(\"monthly_deaths\"),\n",
    "        avg(\"mortality_rate\").alias(\"avg_mortality_rate\"),\n",
    "        count(\"*\").alias(\"counties_reported\"),\n",
    "    )\n",
    "    .orderBy(\"state\", \"year\", \"month\")\n",
    ")\n",
    "\n",
    "# Trigger execution\n",
    "eager_count = eager_result.count()\n",
    "step6_time = time.time() - step6_start\n",
    "print(f\"  Time: {step6_time:.2f}s | Final rows: {eager_count:,}\")\n",
    "\n",
    "eager_total = time.time() - eager_start\n",
    "\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"EAGER APPROACH SUMMARY:\")\n",
    "print(f\"  Total execution time: {eager_total:.2f} seconds\")\n",
    "print(f\"  Number of shuffles: ~4-5 (groupBy, join, filters, final groupBy)\")\n",
    "print(f\"  Data processed: Full dataset multiple times\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Show sample results\n",
    "print(\"\\nSample results (Eager approach):\")\n",
    "eager_result.show(10, truncate=False)\n",
    "\n",
    "# Show execution plan\n",
    "print(\"\\nEXECUTION PLAN (Eager - Notice multiple stages and shuffles):\")\n",
    "eager_result.explain(mode=\"simple\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77c8b8ec-1666-4937-9c92-0be533673ee0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "VERSION 2: LAZY/OPTIMIZED APPROACH (Best Practices)\n",
      "================================================================================\n",
      "Optimizations:\n",
      "  ✅ Filter EARLY (reduce data volume immediately)\n",
      "  ✅ Combine filters together (single pass)\n",
      "  ✅ Join AFTER filtering (smaller dataset)\n",
      "  ✅ Minimize shuffles through intelligent ordering\n",
      "  ✅ Use appropriate partitioning\n",
      "\n",
      "[Step 1] Filtering data FIRST (early reduction)...\n",
      "  Time: 0.00s | Rows after filter: 115,697\n",
      "\n",
      "[Step 2] Adding column transformations on filtered data...\n",
      "  Time: 0.00s\n",
      "\n",
      "[Step 3] GroupBy on filtered dataset...\n",
      "  Time: 0.00s | Rows: 115,697\n",
      "\n",
      "[Step 4] Joining with population data (small dataset)...\n",
      "  Time: 0.00s\n",
      "\n",
      "[Step 5] Final aggregation...\n",
      "  Time: 0.76s | Final rows: 52\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "LAZY/OPTIMIZED APPROACH SUMMARY:\n",
      "  Total execution time: 2.26 seconds\n",
      "  Number of shuffles: ~2 (one groupBy, one final aggregation)\n",
      "  Data processed: Filtered dataset only\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Sample results (Optimized approach):\n",
      "+----------+------+----+-----+----------+-------------+--------------+------------------+-----------------+-------------------+\n",
      "|state     |region|year|month|population|monthly_cases|monthly_deaths|avg_mortality_rate|counties_reported|cases_per_100k     |\n",
      "+----------+------+----+-----+----------+-------------+--------------+------------------+-----------------+-------------------+\n",
      "|California|West  |2020|2    |39538223  |44           |0             |0.0               |4                |0.11128471808153846|\n",
      "|California|West  |2020|3    |39538223  |53893        |1075          |1.9531421311769432|468              |136.3060752629171  |\n",
      "|California|West  |2020|4    |39538223  |864363       |30166         |3.2309658387784093|1234             |2186.1452903434733 |\n",
      "|California|West  |2020|5    |39538223  |2475042      |99347         |3.099645902642947 |1411             |6259.871618408344  |\n",
      "|California|West  |2020|6    |39538223  |4904141      |155680        |2.2489051017364265|1502             |12403.544286752594 |\n",
      "|California|West  |2020|7    |39538223  |11404770     |232621        |1.2139232466490384|1671             |28844.92305079062  |\n",
      "|California|West  |2020|8    |39538223  |19168191     |349458        |1.3111670072947847|1683             |48480.1529901837   |\n",
      "|California|West  |2020|9    |39538223  |23157578     |438311        |1.4751735752790125|1680             |58570.10316320994  |\n",
      "|California|West  |2020|10   |39538223  |27163707     |522654        |1.5523338388611123|1736             |68702.3971714662   |\n",
      "|California|West  |2020|11   |39538223  |31809052     |551791        |1.3805990606541378|1720             |80451.39509684085  |\n",
      "+----------+------+----+-----+----------+-------------+--------------+------------------+-----------------+-------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "EXECUTION PLAN (Optimized - Notice fewer stages):\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- == Initial Plan ==\n",
      "   ColumnarToRow\n",
      "   +- PhotonResultStage\n",
      "      +- PhotonSort [state#14466 ASC NULLS FIRST, year#15874 ASC NULLS FIRST, month#15876 ASC NULLS FIRST]\n",
      "         +- PhotonShuffleExchangeSource\n",
      "            +- PhotonShuffleMapStage ENSURE_REQUIREMENTS, [id=#21171]\n",
      "               +- PhotonShuffleExchangeSink rangepartitioning(state#14466 ASC NULLS FIRST, year#15874 ASC NULLS FIRST, month#15876 ASC NULLS FIRST, 16)\n",
      "                  +- PhotonProject [state#14466, region#15893, year#15874, month#15876, population#15892L, sum(total_cases)#15895L AS monthly_cases#15883L, sum(total_deaths)#15896L AS monthly_deaths#15884L, avg(avg_mortality_rate)#15897 AS avg_mortality_rate#15885, count(1)#15894L AS counties_reported#15886L, ((cast(sum(total_cases)#15895L as double) / cast(population#15892L as double)) * 100000.0) AS cases_per_100k#15899]\n",
      "                     +- PhotonGroupingAgg(keys=[state#14466, region#15893, year#15874, month#15876, population#15892L], functions=[finalmerge_sum(merge sum#15901L) AS sum(total_cases)#15895L, finalmerge_sum(merge sum#15903L) AS sum(total_deaths)#15896L, finalmerge_avg(merge sum#15906, count#15907L) AS avg(avg_mortality_rate)#15897, finalmerge_count(merge count#15909L) AS count(1)#15894L])\n",
      "                        +- PhotonProject [state#14466, region#15893, year#15874, month#15876, population#15892L, sum#15901L, sum#15903L, sum#15906, count#15907L, count#15909L]\n",
      "                           +- PhotonBroadcastHashJoin [state#14466], [state#15891], LeftOuter, BuildRight, false, true\n",
      "                              :- PhotonGroupingAgg(keys=[state#14466, year#15874, month#15876], functions=[partial_sum(total_cases#15879L) AS sum#15901L, partial_sum(total_deaths#15880L) AS sum#15903L, partial_avg(avg_mortality_rate#15881) AS (sum#15906, count#15907L), partial_count(1) AS count#15909L])\n",
      "                              :  +- PhotonGroupingAgg(keys=[state#14466, county#14465, date#14464, year#15874, month#15876], functions=[sum(cases_int#15868), sum(deaths_int#15870), avg(mortality_rate#15872)])\n",
      "                              :     +- PhotonProject [date#14464, county#14465, state#14466, cases_int#15868, deaths_int#15870, CASE WHEN (cases_int#15868 > 0) THEN ((cast(deaths_int#15870 as double) / cast(cases_int#15868 as double)) * 100.0) ELSE 0.0 END AS mortality_rate#15872, year(cast(date#14464 as date)) AS year#15874, month(cast(date#14464 as date)) AS month#15876]\n",
      "                              :        +- PhotonProject [date#14464, county#14465, state#14466, cast(cases#14468 as int) AS cases_int#15868, cast(deaths#14469 as int) AS deaths_int#15870]\n",
      "                              :           +- PhotonShuffleExchangeSource\n",
      "                              :              +- PhotonShuffleMapStage REPARTITION_BY_NUM, [id=#21144]\n",
      "                              :                 +- PhotonShuffleExchangeSink hashpartitioning(state#14466, 4)\n",
      "                              :                    +- PhotonFilter ((((isnotnull(date#14464) AND isnotnull(cases#14468)) AND (year(cast(date#14464 as date)) = 2020)) AND (cast(cases#14468 as bigint) >= 10)) AND state#14466 IN (California,New York,Texas,Florida,Washington))\n",
      "                              :                       +- PhotonRowToColumnar\n",
      "                              :                          +- FileScan csv [date#14464,county#14465,state#14466,cases#14468,deaths#14469] Batched: false, DataFilters: [isnotnull(date#14464), isnotnull(cases#14468), (year(cast(date#14464 as date)) = 2020), (cast(ca..., Format: CSV, Location: InMemoryFileIndex(1 paths)[dbfs:/databricks-datasets/COVID/covid-19-data], PartitionFilters: [], PushedFilters: [IsNotNull(date), IsNotNull(cases), In(state, [California,Florida,New York,Texas,Washington])], ReadSchema: struct<date:string,county:string,state:string,cases:string,deaths:string>\n",
      "                              +- PhotonShuffleExchangeSource\n",
      "                                 +- PhotonShuffleMapStage EXECUTOR_BROADCAST, [id=#21160]\n",
      "                                    +- PhotonShuffleExchangeSink SinglePartition\n",
      "                                       +- PhotonFilter (isnotnull(state#15891) AND state#15891 IN (California,New York,Texas,Florida,Washington))\n",
      "                                          +- PhotonRowToColumnar\n",
      "                                             +- LocalTableScan [state#15891, population#15892L, region#15893]\n",
      "\n",
      "\n",
      "== Photon Explanation ==\n",
      "The query is fully supported by Photon.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# VERSION 2: LAZY/OPTIMIZED APPROACH ✅\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"VERSION 2: LAZY/OPTIMIZED APPROACH (Best Practices)\")\n",
    "print(\"=\" * 80)\n",
    "print(\"Optimizations:\")\n",
    "print(\"  ✅ Filter EARLY (reduce data volume immediately)\")\n",
    "print(\"  ✅ Combine filters together (single pass)\")\n",
    "print(\"  ✅ Join AFTER filtering (smaller dataset)\")\n",
    "print(\"  ✅ Minimize shuffles through intelligent ordering\")\n",
    "print(\"  ✅ Use appropriate partitioning\")\n",
    "\n",
    "lazy_start = time.time()\n",
    "\n",
    "# Step 1: Filter FIRST (GOOD - reduce data early)\n",
    "print(\"\\n[Step 1] Filtering data FIRST (early reduction)...\")\n",
    "step1_start = time.time()\n",
    "lazy_filtered = covid_df.filter(\n",
    "    (year(col(\"date\")) == 2020)\n",
    "    & (col(\"state\").isin(\"California\", \"New York\", \"Texas\", \"Florida\", \"Washington\"))\n",
    "    & (col(\"cases\") >= 10)  # Combine multiple filters\n",
    ")\n",
    "# Repartition after filtering for optimal parallelism\n",
    "lazy_filtered = lazy_filtered.repartition(4, \"state\")\n",
    "step1_time = time.time() - step1_start\n",
    "print(f\"  Time: {step1_time:.2f}s | Rows after filter: {lazy_filtered.count():,}\")\n",
    "\n",
    "# Step 2: Column transformations on FILTERED data (GOOD - fewer rows)\n",
    "print(\"\\n[Step 2] Adding column transformations on filtered data...\")\n",
    "step2_start = time.time()\n",
    "lazy_transformed = (\n",
    "    lazy_filtered.withColumn(\"cases_int\", col(\"cases\").cast(\"integer\"))\n",
    "    .withColumn(\"deaths_int\", col(\"deaths\").cast(\"integer\"))\n",
    "    .withColumn(\n",
    "        \"mortality_rate\",\n",
    "        when(\n",
    "            col(\"cases_int\") > 0, (col(\"deaths_int\") / col(\"cases_int\")) * 100\n",
    "        ).otherwise(0),\n",
    "    )\n",
    "    .withColumn(\"year\", year(col(\"date\")))\n",
    "    .withColumn(\"month\", month(col(\"date\")))\n",
    "    .withColumn(\n",
    "        \"case_category\",\n",
    "        when(col(\"cases_int\") < 100, \"Low\")\n",
    "        .when(col(\"cases_int\") < 1000, \"Medium\")\n",
    "        .otherwise(\"High\"),\n",
    "    )\n",
    ")\n",
    "step2_time = time.time() - step2_start\n",
    "print(f\"  Time: {step2_time:.2f}s\")\n",
    "\n",
    "# Step 3: GroupBy on FILTERED data (GOOD - much less data to shuffle)\n",
    "print(\"\\n[Step 3] GroupBy on filtered dataset...\")\n",
    "step3_start = time.time()\n",
    "lazy_grouped = lazy_transformed.groupBy(\"state\", \"county\", \"date\", \"year\", \"month\").agg(\n",
    "    sum(\"cases_int\").alias(\"total_cases\"),\n",
    "    sum(\"deaths_int\").alias(\"total_deaths\"),\n",
    "    avg(\"mortality_rate\").alias(\"avg_mortality_rate\"),\n",
    "    first(\"case_category\").alias(\"case_category\"),\n",
    ")\n",
    "step3_time = time.time() - step3_start\n",
    "print(f\"  Time: {step3_time:.2f}s | Rows: {lazy_grouped.count():,}\")\n",
    "\n",
    "# Step 4: Join on SMALL dataset (GOOD - broadcast join possible)\n",
    "print(\"\\n[Step 4] Joining with population data (small dataset)...\")\n",
    "step4_start = time.time()\n",
    "# Broadcast the small dimension table\n",
    "lazy_joined = lazy_grouped.join(broadcast(state_population), on=\"state\", how=\"left\")\n",
    "step4_time = time.time() - step4_start\n",
    "print(f\"  Time: {step4_time:.2f}s\")\n",
    "\n",
    "# Step 5: Final aggregation (GOOD - single optimized operation)\n",
    "print(\"\\n[Step 5] Final aggregation...\")\n",
    "step5_start = time.time()\n",
    "lazy_result = (\n",
    "    lazy_joined.groupBy(\"state\", \"region\", \"year\", \"month\", \"population\")\n",
    "    .agg(\n",
    "        sum(\"total_cases\").alias(\"monthly_cases\"),\n",
    "        sum(\"total_deaths\").alias(\"monthly_deaths\"),\n",
    "        avg(\"avg_mortality_rate\").alias(\"avg_mortality_rate\"),\n",
    "        count(\"*\").alias(\"counties_reported\"),\n",
    "    )\n",
    "    .withColumn(\"cases_per_100k\", (col(\"monthly_cases\") / col(\"population\")) * 100000)\n",
    "    .orderBy(\"state\", \"year\", \"month\")\n",
    ")\n",
    "\n",
    "# Trigger execution\n",
    "lazy_count = lazy_result.count()\n",
    "step5_time = time.time() - step5_start\n",
    "print(f\"  Time: {step5_time:.2f}s | Final rows: {lazy_count:,}\")\n",
    "\n",
    "lazy_total = time.time() - lazy_start\n",
    "\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"LAZY/OPTIMIZED APPROACH SUMMARY:\")\n",
    "print(f\"  Total execution time: {lazy_total:.2f} seconds\")\n",
    "print(f\"  Number of shuffles: ~2 (one groupBy, one final aggregation)\")\n",
    "print(f\"  Data processed: Filtered dataset only\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Show sample results\n",
    "print(\"\\nSample results (Optimized approach):\")\n",
    "lazy_result.show(10, truncate=False)\n",
    "\n",
    "# Show execution plan\n",
    "print(\"\\nEXECUTION PLAN (Optimized - Notice fewer stages):\")\n",
    "lazy_result.explain(mode=\"simple\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25f6c5b5-bc28-4695-bb16-9e0e029782e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PERFORMANCE COMPARISON\n",
      "================================================================================\n",
      "Eager/Inefficient Approach:   4.29 seconds\n",
      "Lazy/Optimized Approach:   2.26 seconds\n",
      "Time Saved:   2.03 seconds\n",
      "Speedup:   1.90x faster\n",
      "Performance Improvement:   47.4%\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# PERFORMANCE COMPARISON\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PERFORMANCE COMPARISON\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "speedup = eager_total / lazy_total\n",
    "time_saved = eager_total - lazy_total\n",
    "percent_improvement = ((eager_total - lazy_total) / eager_total) * 100\n",
    "\n",
    "print(f\"Eager/Inefficient Approach: {eager_total:6.2f} seconds\")\n",
    "print(f\"Lazy/Optimized Approach: {lazy_total:6.2f} seconds\")\n",
    "print(f\"Time Saved: {time_saved:6.2f} seconds\")\n",
    "print(f\"Speedup: {speedup:6.2f}x faster\")\n",
    "print(f\"Performance Improvement: {percent_improvement:6.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fea27d21-92c6-446e-9bc2-8d6aae8165b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# SQL QUERYING\n",
    "# Make sure we have data in the correct format (extra check)\n",
    "# First, cast the string columns to integers in the original dataframe\n",
    "covid_df = covid_df.withColumn(\"cases\", col(\"cases\").cast(\"integer\")).withColumn(\n",
    "    \"deaths\", col(\"deaths\").cast(\"integer\")\n",
    ")\n",
    "\n",
    "# Cast columns and add transformations\n",
    "enriched_df = (\n",
    "    covid_df.withColumn(\n",
    "        \"mortality_rate\",\n",
    "        when(col(\"cases\") > 0, (col(\"deaths\") / col(\"cases\")) * 100).otherwise(0),\n",
    "    )\n",
    "    .withColumn(\"month\", month(col(\"date\")))\n",
    "    .withColumn(\"year\", year(col(\"date\")))\n",
    ")\n",
    "\n",
    "# Create state_stats view with properly typed columns\n",
    "state_stats = enriched_df.groupBy(\"state\", \"county\").agg(\n",
    "    sum(\"cases\").alias(\"total_cases\"),\n",
    "    sum(\"deaths\").alias(\"total_deaths\"),\n",
    "    avg(\"mortality_rate\").alias(\"avg_mortality_rate\"),\n",
    "    count(\"*\").alias(\"days_reported\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "108f0f88-0dd7-4a79-bed4-246a05cdd3d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "SQL QUERY 1: TOP 10 COUNTIES WITH HIGHEST MORTALITY RATES\n",
      "================================================================================\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "VERSION 1A: EAGER/INEFFICIENT APPROACH\n",
      "--------------------------------------------------------------------------------\n",
      "Problems:\n",
      "  ❌ Computes statistics on ALL counties first\n",
      "  ❌ Filters AFTER expensive aggregation\n",
      "  ❌ No predicate pushdown\n",
      "  ❌ Processes unnecessary data\n",
      "\n",
      "[Executing inefficient query...]\n",
      "⏱️  Eager Query 1 Time: 0.16685 seconds\n",
      "\n",
      "Execution Plan (Eager - notice full table scan and window function):\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- == Initial Plan ==\n",
      "   ColumnarToRow\n",
      "   +- PhotonResultStage\n",
      "      +- PhotonProject [state#14466, county#14465, total_cases#16491L, total_deaths#16492L, round(avg_mortality_rate#16493, 2) AS mortality_rate_pct#16894, days_reported#16494L]\n",
      "         +- PhotonFilter ((isnotnull(total_cases#16491L) AND (rank#16893 <= 10)) AND (total_cases#16491L >= 100))\n",
      "            +- PhotonWindow [state#14466, county#14465, total_cases#16491L, total_deaths#16492L, avg_mortality_rate#16493, days_reported#16494L, row_number() windowspecdefinition(avg_mortality_rate#16493 DESC NULLS LAST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS rank#16893]\n",
      "               +- PhotonTopK(sortOrder=[avg_mortality_rate#16493 DESC NULLS LAST], partitionOrderCount=0)\n",
      "                  +- PhotonShuffleExchangeSource\n",
      "                     +- PhotonShuffleMapStage ENSURE_REQUIREMENTS, [id=#24503]\n",
      "                        +- PhotonShuffleExchangeSink SinglePartition\n",
      "                           +- PhotonProject [state#14466, county#14465, total_cases#16491L, total_deaths#16492L, avg_mortality_rate#16493, days_reported#16494L]\n",
      "                              +- PhotonFilter (_local_row_number#16912 <= 10)\n",
      "                                 +- PhotonWindow [state#14466, county#14465, total_cases#16491L, total_deaths#16492L, avg_mortality_rate#16493, days_reported#16494L, row_number() windowspecdefinition(avg_mortality_rate#16493 DESC NULLS LAST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS _local_row_number#16912]\n",
      "                                    +- PhotonTopK(sortOrder=[avg_mortality_rate#16493 DESC NULLS LAST], partitionOrderCount=0)\n",
      "                                       +- PhotonGroupingAgg(keys=[state#14466, county#14465], functions=[finalmerge_sum(merge sum#16897L) AS sum(cases)#16885L, finalmerge_sum(merge sum#16899L) AS sum(deaths)#16886L, finalmerge_avg(merge sum#16902, count#16903L) AS avg(mortality_rate)#16887, finalmerge_count(merge count#16905L) AS count(1)#16884L])\n",
      "                                          +- PhotonShuffleExchangeSource\n",
      "                                             +- PhotonShuffleMapStage ENSURE_REQUIREMENTS, [id=#24470]\n",
      "                                                +- PhotonShuffleExchangeSink hashpartitioning(state#14466, county#14465, 1024)\n",
      "                                                   +- PhotonGroupingAgg(keys=[state#14466, county#14465], functions=[partial_sum(cases#16480) AS sum#16897L, partial_sum(deaths#16482) AS sum#16899L, partial_avg(mortality_rate#16485) AS (sum#16902, count#16903L), partial_count(1) AS count#16905L])\n",
      "                                                      +- PhotonProject [county#14465, state#14466, cases#16480, deaths#16482, CASE WHEN (cases#16480 > 0) THEN ((cast(deaths#16482 as double) / cast(cases#16480 as double)) * 100.0) ELSE 0.0 END AS mortality_rate#16485]\n",
      "                                                         +- PhotonProject [county#14465, state#14466, cast(cases#14468 as int) AS cases#16480, cast(deaths#14469 as int) AS deaths#16482]\n",
      "                                                            +- PhotonRowToColumnar\n",
      "                                                               +- FileScan csv [county#14465,state#14466,cases#14468,deaths#14469] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[dbfs:/databricks-datasets/COVID/covid-19-data], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<county:string,state:string,cases:string,deaths:string>\n",
      "\n",
      "\n",
      "== Photon Explanation ==\n",
      "The query is fully supported by Photon.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# SQL QUERY 1: TOP 10 COUNTIES WITH HIGHEST MORTALITY RATES\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SQL QUERY 1: TOP 10 COUNTIES WITH HIGHEST MORTALITY RATES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# VERSION 1A: EAGER/INEFFICIENT SQL QUERY ❌\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"VERSION 1A: EAGER/INEFFICIENT APPROACH\")\n",
    "print(\"-\" * 80)\n",
    "print(\"Problems:\")\n",
    "print(\"  ❌ Computes statistics on ALL counties first\")\n",
    "print(\"  ❌ Filters AFTER expensive aggregation\")\n",
    "print(\"  ❌ No predicate pushdown\")\n",
    "print(\"  ❌ Processes unnecessary data\")\n",
    "\n",
    "eager_q1_start = time.time()\n",
    "\n",
    "# Register view\n",
    "state_stats.createOrReplaceTempView(\"state_stats_eager\")\n",
    "\n",
    "# INEFFICIENT: No filtering before aggregation, sorting all data\n",
    "sql_eager_q1 = \"\"\"\n",
    "    SELECT \n",
    "        state,\n",
    "        county,\n",
    "        total_cases,\n",
    "        total_deaths,\n",
    "        ROUND(avg_mortality_rate, 2) as mortality_rate_pct,\n",
    "        days_reported\n",
    "    FROM (\n",
    "        SELECT \n",
    "            state,\n",
    "            county,\n",
    "            total_cases,\n",
    "            total_deaths,\n",
    "            avg_mortality_rate,\n",
    "            days_reported,\n",
    "            ROW_NUMBER() OVER (ORDER BY avg_mortality_rate DESC) as rank\n",
    "        FROM state_stats_eager\n",
    "    ) ranked\n",
    "    WHERE total_cases >= 100 AND rank <= 10\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n[Executing inefficient query...]\")\n",
    "eager_result_q1 = spark.sql(sql_eager_q1)\n",
    "eager_q1_time = time.time() - eager_q1_start\n",
    "\n",
    "print(f\"⏱️  Eager Query 1 Time: {eager_q1_time:.5f} seconds\")\n",
    "\n",
    "print(\"\\nExecution Plan (Eager - notice full table scan and window function):\")\n",
    "eager_result_q1.explain(mode=\"simple\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "76a054a5-af78-4dd1-870d-eb52c1f9f7ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------------------------------------\n",
      "VERSION 1B: LAZY/OPTIMIZED APPROACH\n",
      "--------------------------------------------------------------------------------\n",
      "Optimizations:\n",
      "  ✅ Filters applied early (total_cases >= 100)\n",
      "  ✅ Uses LIMIT instead of window function\n",
      "  ✅ Predicate pushdown reduces data scanned\n",
      "  ✅ Simplified query plan\n",
      "\n",
      "[Executing optimized query...]\n",
      "⏱️  Lazy Query 1 Time: 0.15117 seconds\n",
      "\n",
      "Execution Plan (Optimized - notice simpler plan):\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- == Initial Plan ==\n",
      "   ColumnarToRow\n",
      "   +- PhotonResultStage\n",
      "      +- PhotonProject [state#14466, county#14465, total_cases#16491L, total_deaths#16492L, mortality_rate_pct#17136, days_reported#16494L]\n",
      "         +- PhotonTopK(sortOrder=[avg_mortality_rate#16493 DESC NULLS LAST], partitionOrderCount=0)\n",
      "            +- PhotonShuffleExchangeSource\n",
      "               +- PhotonShuffleMapStage ENSURE_REQUIREMENTS, [id=#25223]\n",
      "                  +- PhotonShuffleExchangeSink SinglePartition\n",
      "                     +- PhotonTopK(sortOrder=[avg_mortality_rate#16493 DESC NULLS LAST], partitionOrderCount=0)\n",
      "                        +- PhotonProject [state#14466, county#14465, total_cases#16491L, total_deaths#16492L, round(avg_mortality_rate#16493, 2) AS mortality_rate_pct#17136, days_reported#16494L, avg_mortality_rate#16493]\n",
      "                           +- PhotonFilter (isnotnull(total_cases#16491L) AND (total_cases#16491L >= 100))\n",
      "                              +- PhotonGroupingAgg(keys=[state#14466, county#14465], functions=[finalmerge_sum(merge sum#17138L) AS sum(cases)#17130L, finalmerge_sum(merge sum#17140L) AS sum(deaths)#17131L, finalmerge_avg(merge sum#17143, count#17144L) AS avg(mortality_rate)#17132, finalmerge_count(merge count#17146L) AS count(1)#17129L])\n",
      "                                 +- PhotonShuffleExchangeSource\n",
      "                                    +- PhotonShuffleMapStage ENSURE_REQUIREMENTS, [id=#25211]\n",
      "                                       +- PhotonShuffleExchangeSink hashpartitioning(state#14466, county#14465, 1024)\n",
      "                                          +- PhotonGroupingAgg(keys=[state#14466, county#14465], functions=[partial_sum(cases#16480) AS sum#17138L, partial_sum(deaths#16482) AS sum#17140L, partial_avg(mortality_rate#16485) AS (sum#17143, count#17144L), partial_count(1) AS count#17146L])\n",
      "                                             +- PhotonProject [county#14465, state#14466, cases#16480, deaths#16482, CASE WHEN (cases#16480 > 0) THEN ((cast(deaths#16482 as double) / cast(cases#16480 as double)) * 100.0) ELSE 0.0 END AS mortality_rate#16485]\n",
      "                                                +- PhotonProject [county#14465, state#14466, cast(cases#14468 as int) AS cases#16480, cast(deaths#14469 as int) AS deaths#16482]\n",
      "                                                   +- PhotonRowToColumnar\n",
      "                                                      +- FileScan csv [county#14465,state#14466,cases#14468,deaths#14469] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[dbfs:/databricks-datasets/COVID/covid-19-data], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<county:string,state:string,cases:string,deaths:string>\n",
      "\n",
      "\n",
      "== Photon Explanation ==\n",
      "The query is fully supported by Photon.\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# VERSION 1B: LAZY/OPTIMIZED SQL QUERY ✅\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"VERSION 1B: LAZY/OPTIMIZED APPROACH\")\n",
    "print(\"-\" * 80)\n",
    "print(\"Optimizations:\")\n",
    "print(\"  ✅ Filters applied early (total_cases >= 100)\")\n",
    "print(\"  ✅ Uses LIMIT instead of window function\")\n",
    "print(\"  ✅ Predicate pushdown reduces data scanned\")\n",
    "print(\"  ✅ Simplified query plan\")\n",
    "\n",
    "lazy_q1_start = time.time()\n",
    "\n",
    "# Register view\n",
    "state_stats.createOrReplaceTempView(\"state_stats_lazy\")\n",
    "\n",
    "# OPTIMIZED: Filter early, simple ORDER BY with LIMIT\n",
    "sql_lazy_q1 = \"\"\"\n",
    "    SELECT \n",
    "        state,\n",
    "        county,\n",
    "        total_cases,\n",
    "        total_deaths,\n",
    "        ROUND(avg_mortality_rate, 2) as mortality_rate_pct,\n",
    "        days_reported\n",
    "    FROM state_stats_lazy\n",
    "    WHERE total_cases >= 100\n",
    "    ORDER BY avg_mortality_rate DESC\n",
    "    LIMIT 10\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n[Executing optimized query...]\")\n",
    "lazy_result_q1 = spark.sql(sql_lazy_q1)\n",
    "lazy_q1_time = time.time() - lazy_q1_start\n",
    "\n",
    "print(f\"⏱️  Lazy Query 1 Time: {lazy_q1_time:.5f} seconds\")\n",
    "\n",
    "print(\"\\nExecution Plan (Optimized - notice simpler plan):\")\n",
    "lazy_result_q1.explain(mode=\"simple\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a95b9782-b2cc-454f-b7d9-287ba888f14f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PERFORMANCE COMPARISON\n",
      "================================================================================\n",
      "Eager/Inefficient Approach:   0.17 seconds\n",
      "Lazy/Optimized Approach:   0.15 seconds\n",
      "Time Saved:   0.02 seconds\n",
      "Speedup:   1.10x faster\n",
      "Performance Improvement:    9.4%\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# PERFORMANCE COMPARISON\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PERFORMANCE COMPARISON\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "speedup_q1 = eager_q1_time / lazy_q1_time\n",
    "time_saved_q1 = eager_q1_time - lazy_q1_time\n",
    "percent_improvement_q1 = ((eager_q1_time - lazy_q1_time) / eager_q1_time) * 100\n",
    "\n",
    "print(f\"Eager/Inefficient Approach: {eager_q1_time:6.2f} seconds\")\n",
    "print(f\"Lazy/Optimized Approach: {lazy_q1_time:6.2f} seconds\")\n",
    "print(f\"Time Saved: {time_saved_q1:6.2f} seconds\")\n",
    "print(f\"Speedup: {speedup_q1:6.2f}x faster\")\n",
    "print(f\"Performance Improvement: {percent_improvement_q1:6.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed419fc0-c388-4a10-9748-03de6ac3fb67",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "SQL QUERY 2: STATE-LEVEL COMPARISON WITH AGGREGATIONS\n",
      "================================================================================\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "VERSION 2A: EAGER/INEFFICIENT APPROACH\n",
      "--------------------------------------------------------------------------------\n",
      "Problems:\n",
      "  ❌ Multiple subqueries with repeated aggregations\n",
      "  ❌ Computes aggregations separately (inefficient)\n",
      "  ❌ Multiple passes over the same data\n",
      "  ❌ Unnecessary intermediate results\n",
      "\n",
      "[Executing inefficient query...]\n",
      "⏱️  Eager Query 2 Time: 0.80 seconds\n",
      "\n",
      "Execution Plan (Eager - notice multiple scans and joins):\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- == Initial Plan ==\n",
      "   ColumnarToRow\n",
      "   +- PhotonResultStage\n",
      "      +- PhotonSort [state_total_cases#17617L DESC NULLS LAST]\n",
      "         +- PhotonShuffleExchangeSource\n",
      "            +- PhotonShuffleMapStage ENSURE_REQUIREMENTS, [id=#27470]\n",
      "               +- PhotonShuffleExchangeSink rangepartitioning(state_total_cases#17617L DESC NULLS LAST, 1024)\n",
      "                  +- PhotonProject [state#14466, num_counties#17616L, state_total_cases#17617L, state_total_deaths#17618L, avg_mortality_rate#17619, max_county_cases#17620L]\n",
      "                     +- PhotonShuffledHashJoin [state#14466], [state#17641], LeftOuter, BuildRight\n",
      "                        :- PhotonProject [state#14466, num_counties#17616L, state_total_cases#17617L, state_total_deaths#17618L, avg_mortality_rate#17619]\n",
      "                        :  +- PhotonShuffledHashJoin [state#14466], [state#17635], LeftOuter, BuildRight\n",
      "                        :     :- PhotonProject [state#14466, num_counties#17616L, state_total_cases#17617L, state_total_deaths#17618L]\n",
      "                        :     :  +- PhotonShuffledHashJoin [state#14466], [state#17629], LeftOuter, BuildRight\n",
      "                        :     :     :- PhotonProject [state#14466, num_counties#17616L, state_total_cases#17617L]\n",
      "                        :     :     :  +- PhotonShuffledHashJoin [state#14466], [state#17623], LeftOuter, BuildRight\n",
      "                        :     :     :     :- PhotonGroupingAgg(keys=[state#14466], functions=[finalmerge_count(distinct merge count#17655L) AS count(county)#17645L])\n",
      "                        :     :     :     :  +- PhotonShuffleExchangeSource\n",
      "                        :     :     :     :     +- PhotonShuffleMapStage ENSURE_REQUIREMENTS, [id=#27370]\n",
      "                        :     :     :     :        +- PhotonShuffleExchangeSink hashpartitioning(state#14466, 1024)\n",
      "                        :     :     :     :           +- PhotonGroupingAgg(keys=[state#14466], functions=[partial_count(distinct county#14465) AS count#17655L])\n",
      "                        :     :     :     :              +- PhotonGroupingAgg(keys=[state#14466, county#14465], functions=[])\n",
      "                        :     :     :     :                 +- PhotonShuffleExchangeSource\n",
      "                        :     :     :     :                    +- PhotonShuffleMapStage ENSURE_REQUIREMENTS, [id=#27362]\n",
      "                        :     :     :     :                       +- PhotonShuffleExchangeSink hashpartitioning(state#14466, county#14465, 1024)\n",
      "                        :     :     :     :                          +- PhotonGroupingAgg(keys=[state#14466, county#14465], functions=[])\n",
      "                        :     :     :     :                             +- PhotonRowToColumnar\n",
      "                        :     :     :     :                                +- FileScan csv [county#14465,state#14466] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[dbfs:/databricks-datasets/COVID/covid-19-data], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<county:string,state:string>\n",
      "                        :     :     :     +- PhotonGroupingAgg(keys=[state#17623], functions=[finalmerge_sum(merge sum#17657L) AS sum(total_cases)#17646L])\n",
      "                        :     :     :        +- PhotonShuffleExchangeSource\n",
      "                        :     :     :           +- PhotonShuffleMapStage ENSURE_REQUIREMENTS, [id=#27383]\n",
      "                        :     :     :              +- PhotonShuffleExchangeSink hashpartitioning(state#17623, 1024)\n",
      "                        :     :     :                 +- PhotonGroupingAgg(keys=[state#17623], functions=[partial_sum(total_cases#16491L) AS sum#17657L])\n",
      "                        :     :     :                    +- PhotonProject [state#17623, cast(cast(cases#17625 as int) as bigint) AS total_cases#16491L]\n",
      "                        :     :     :                       +- PhotonFilter isnotnull(state#17623)\n",
      "                        :     :     :                          +- PhotonRowToColumnar\n",
      "                        :     :     :                             +- FileScan csv [state#17623,cases#17625] Batched: false, DataFilters: [isnotnull(state#17623)], Format: CSV, Location: InMemoryFileIndex(1 paths)[dbfs:/databricks-datasets/COVID/covid-19-data], PartitionFilters: [], PushedFilters: [IsNotNull(state)], ReadSchema: struct<state:string,cases:string>\n",
      "                        :     :     +- PhotonGroupingAgg(keys=[state#17629], functions=[finalmerge_sum(merge sum#17660L) AS sum(total_deaths)#17647L])\n",
      "                        :     :        +- PhotonShuffleExchangeSource\n",
      "                        :     :           +- PhotonShuffleMapStage ENSURE_REQUIREMENTS, [id=#27403]\n",
      "                        :     :              +- PhotonShuffleExchangeSink hashpartitioning(state#17629, 1024)\n",
      "                        :     :                 +- PhotonGroupingAgg(keys=[state#17629], functions=[partial_sum(total_deaths#16492L) AS sum#17660L])\n",
      "                        :     :                    +- PhotonProject [state#17629, cast(cast(deaths#17632 as int) as bigint) AS total_deaths#16492L]\n",
      "                        :     :                       +- PhotonFilter isnotnull(state#17629)\n",
      "                        :     :                          +- PhotonRowToColumnar\n",
      "                        :     :                             +- FileScan csv [state#17629,deaths#17632] Batched: false, DataFilters: [isnotnull(state#17629)], Format: CSV, Location: InMemoryFileIndex(1 paths)[dbfs:/databricks-datasets/COVID/covid-19-data], PartitionFilters: [], PushedFilters: [IsNotNull(state)], ReadSchema: struct<state:string,deaths:string>\n",
      "                        :     +- PhotonGroupingAgg(keys=[state#17635], functions=[finalmerge_avg(merge sum#17664, count#17665L) AS avg(avg_mortality_rate)#17648])\n",
      "                        :        +- PhotonShuffleExchangeSource\n",
      "                        :           +- PhotonShuffleMapStage ENSURE_REQUIREMENTS, [id=#27431]\n",
      "                        :              +- PhotonShuffleExchangeSink hashpartitioning(state#17635, 1024)\n",
      "                        :                 +- PhotonGroupingAgg(keys=[state#17635], functions=[partial_avg(avg_mortality_rate#16493) AS (sum#17664, count#17665L)])\n",
      "                        :                    +- PhotonGroupingAgg(keys=[state#17635, county#17634], functions=[finalmerge_avg(merge sum#16902, count#16903L) AS avg(mortality_rate)#16887])\n",
      "                        :                       +- PhotonShuffleExchangeSource\n",
      "                        :                          +- PhotonShuffleMapStage ENSURE_REQUIREMENTS, [id=#27423]\n",
      "                        :                             +- PhotonShuffleExchangeSink hashpartitioning(state#17635, county#17634, 1024)\n",
      "                        :                                +- PhotonGroupingAgg(keys=[state#17635, county#17634], functions=[partial_avg(mortality_rate#16485) AS (sum#16902, count#16903L)])\n",
      "                        :                                   +- PhotonProject [county#17634, state#17635, CASE WHEN (cases#16480 > 0) THEN ((cast(deaths#16482 as double) / cast(cases#16480 as double)) * 100.0) ELSE 0.0 END AS mortality_rate#16485]\n",
      "                        :                                      +- PhotonProject [county#17634, state#17635, cast(cases#17637 as int) AS cases#16480, cast(deaths#17638 as int) AS deaths#16482]\n",
      "                        :                                         +- PhotonFilter isnotnull(state#17635)\n",
      "                        :                                            +- PhotonRowToColumnar\n",
      "                        :                                               +- FileScan csv [county#17634,state#17635,cases#17637,deaths#17638] Batched: false, DataFilters: [isnotnull(state#17635)], Format: CSV, Location: InMemoryFileIndex(1 paths)[dbfs:/databricks-datasets/COVID/covid-19-data], PartitionFilters: [], PushedFilters: [IsNotNull(state)], ReadSchema: struct<county:string,state:string,cases:string,deaths:string>\n",
      "                        +- PhotonGroupingAgg(keys=[state#17641], functions=[finalmerge_max(merge max#17671L) AS max(total_cases)#17649L])\n",
      "                           +- PhotonShuffleExchangeSource\n",
      "                              +- PhotonShuffleMapStage ENSURE_REQUIREMENTS, [id=#27458]\n",
      "                                 +- PhotonShuffleExchangeSink hashpartitioning(state#17641, 1024)\n",
      "                                    +- PhotonGroupingAgg(keys=[state#17641], functions=[partial_max(total_cases#16491L) AS max#17671L])\n",
      "                                       +- PhotonGroupingAgg(keys=[state#17641, county#17640], functions=[finalmerge_sum(merge sum#16897L) AS sum(cases)#16885L])\n",
      "                                          +- PhotonShuffleExchangeSource\n",
      "                                             +- PhotonShuffleMapStage ENSURE_REQUIREMENTS, [id=#27450]\n",
      "                                                +- PhotonShuffleExchangeSink hashpartitioning(state#17641, county#17640, 1024)\n",
      "                                                   +- PhotonGroupingAgg(keys=[state#17641, county#17640], functions=[partial_sum(cases#16480) AS sum#16897L])\n",
      "                                                      +- PhotonProject [county#17640, state#17641, cast(cases#17643 as int) AS cases#16480]\n",
      "                                                         +- PhotonFilter isnotnull(state#17641)\n",
      "                                                            +- PhotonRowToColumnar\n",
      "                                                               +- FileScan csv [county#17640,state#17641,cases#17643] Batched: false, DataFilters: [isnotnull(state#17641)], Format: CSV, Location: InMemoryFileIndex(1 paths)[dbfs:/databricks-datasets/COVID/covid-19-data], PartitionFilters: [], PushedFilters: [IsNotNull(state)], ReadSchema: struct<county:string,state:string,cases:string>\n",
      "\n",
      "\n",
      "== Photon Explanation ==\n",
      "The query is fully supported by Photon.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# SQL QUERY 2: STATE-LEVEL COMPARISON\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SQL QUERY 2: STATE-LEVEL COMPARISON WITH AGGREGATIONS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# VERSION 2A: EAGER/INEFFICIENT SQL QUERY ❌\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"VERSION 2A: EAGER/INEFFICIENT APPROACH\")\n",
    "print(\"-\" * 80)\n",
    "print(\"Problems:\")\n",
    "print(\"  ❌ Multiple subqueries with repeated aggregations\")\n",
    "print(\"  ❌ Computes aggregations separately (inefficient)\")\n",
    "print(\"  ❌ Multiple passes over the same data\")\n",
    "print(\"  ❌ Unnecessary intermediate results\")\n",
    "\n",
    "eager_q2_start = time.time()\n",
    "\n",
    "# INEFFICIENT: Multiple subqueries, repeated computations\n",
    "sql_eager_q2 = \"\"\"\n",
    "    SELECT \n",
    "        s1.state,\n",
    "        s1.num_counties,\n",
    "        s2.state_total_cases,\n",
    "        s3.state_total_deaths,\n",
    "        s4.avg_mortality_rate,\n",
    "        s5.max_county_cases\n",
    "    FROM (\n",
    "        SELECT state, COUNT(DISTINCT county) as num_counties\n",
    "        FROM state_stats_eager\n",
    "        GROUP BY state\n",
    "    ) s1\n",
    "    LEFT JOIN (\n",
    "        SELECT state, SUM(total_cases) as state_total_cases\n",
    "        FROM state_stats_eager\n",
    "        GROUP BY state\n",
    "    ) s2 ON s1.state = s2.state\n",
    "    LEFT JOIN (\n",
    "        SELECT state, SUM(total_deaths) as state_total_deaths\n",
    "        FROM state_stats_eager\n",
    "        GROUP BY state\n",
    "    ) s3 ON s1.state = s3.state\n",
    "    LEFT JOIN (\n",
    "        SELECT state, ROUND(AVG(avg_mortality_rate), 2) as avg_mortality_rate\n",
    "        FROM state_stats_eager\n",
    "        GROUP BY state\n",
    "    ) s4 ON s1.state = s4.state\n",
    "    LEFT JOIN (\n",
    "        SELECT state, MAX(total_cases) as max_county_cases\n",
    "        FROM state_stats_eager\n",
    "        GROUP BY state\n",
    "    ) s5 ON s1.state = s5.state\n",
    "    ORDER BY s2.state_total_cases DESC\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n[Executing inefficient query...]\")\n",
    "eager_result_q2 = spark.sql(sql_eager_q2)\n",
    "eager_count_q2 = eager_result_q2.count()\n",
    "eager_q2_time = time.time() - eager_q2_start\n",
    "\n",
    "print(f\"⏱️  Eager Query 2 Time: {eager_q2_time:.2f} seconds\")\n",
    "\n",
    "print(\"\\nExecution Plan (Eager - notice multiple scans and joins):\")\n",
    "eager_result_q2.explain(mode=\"simple\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ec7e108-dab7-4c8a-9628-0da1e394f0f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------------------------------------\n",
      "VERSION 2B: LAZY/OPTIMIZED APPROACH\n",
      "--------------------------------------------------------------------------------\n",
      "Optimizations:\n",
      "  ✅ Single GROUP BY with all aggregations together\n",
      "  ✅ One pass over data\n",
      "  ✅ No unnecessary joins\n",
      "  ✅ Efficient execution plan\n",
      "\n",
      "[Executing optimized query...]\n",
      "⏱️  Lazy Query 2 Time: 0.73 seconds\n",
      "\n",
      "Execution Plan (Optimized - notice single scan):\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- == Initial Plan ==\n",
      "   ColumnarToRow\n",
      "   +- PhotonResultStage\n",
      "      +- PhotonSort [state_total_cases#17835L DESC NULLS LAST]\n",
      "         +- PhotonShuffleExchangeSource\n",
      "            +- PhotonShuffleMapStage ENSURE_REQUIREMENTS, [id=#28003]\n",
      "               +- PhotonShuffleExchangeSink rangepartitioning(state_total_cases#17835L DESC NULLS LAST, 1024)\n",
      "                  +- PhotonGroupingAgg(keys=[state#14466], functions=[finalmerge_sum(merge sum#17847L) AS sum(total_cases)#17840L, finalmerge_sum(merge sum#17849L) AS sum(total_deaths)#17841L, finalmerge_avg(merge sum#17852, count#17853L) AS avg(avg_mortality_rate)#17842, finalmerge_max(merge max#17855L) AS max(total_cases)#17843L, finalmerge_count(distinct merge count#17845L) AS count(county)#17839L])\n",
      "                     +- PhotonShuffleExchangeSource\n",
      "                        +- PhotonShuffleMapStage ENSURE_REQUIREMENTS, [id=#27997]\n",
      "                           +- PhotonShuffleExchangeSink hashpartitioning(state#14466, 1024)\n",
      "                              +- PhotonGroupingAgg(keys=[state#14466], functions=[merge_sum(merge sum#17847L) AS sum#17847L, merge_sum(merge sum#17849L) AS sum#17849L, merge_avg(merge sum#17852, count#17853L) AS (sum#17852, count#17853L), merge_max(merge max#17855L) AS max#17855L, partial_count(distinct county#14465) AS count#17845L])\n",
      "                                 +- PhotonGroupingAgg(keys=[state#14466, county#14465], functions=[merge_sum(merge sum#17847L) AS sum#17847L, merge_sum(merge sum#17849L) AS sum#17849L, merge_avg(merge sum#17852, count#17853L) AS (sum#17852, count#17853L), merge_max(merge max#17855L) AS max#17855L])\n",
      "                                    +- PhotonGroupingAgg(keys=[state#14466, county#14465], functions=[partial_sum(total_cases#16491L) AS sum#17847L, partial_sum(total_deaths#16492L) AS sum#17849L, partial_avg(avg_mortality_rate#16493) AS (sum#17852, count#17853L), partial_max(total_cases#16491L) AS max#17855L])\n",
      "                                       +- PhotonGroupingAgg(keys=[state#14466, county#14465], functions=[finalmerge_sum(merge sum#17138L) AS sum(cases)#17130L, finalmerge_sum(merge sum#17140L) AS sum(deaths)#17131L, finalmerge_avg(merge sum#17143, count#17144L) AS avg(mortality_rate)#17132])\n",
      "                                          +- PhotonShuffleExchangeSource\n",
      "                                             +- PhotonShuffleMapStage ENSURE_REQUIREMENTS, [id=#27985]\n",
      "                                                +- PhotonShuffleExchangeSink hashpartitioning(state#14466, county#14465, 1024)\n",
      "                                                   +- PhotonGroupingAgg(keys=[state#14466, county#14465], functions=[partial_sum(cases#16480) AS sum#17138L, partial_sum(deaths#16482) AS sum#17140L, partial_avg(mortality_rate#16485) AS (sum#17143, count#17144L)])\n",
      "                                                      +- PhotonProject [county#14465, state#14466, cases#16480, deaths#16482, CASE WHEN (cases#16480 > 0) THEN ((cast(deaths#16482 as double) / cast(cases#16480 as double)) * 100.0) ELSE 0.0 END AS mortality_rate#16485]\n",
      "                                                         +- PhotonProject [county#14465, state#14466, cast(cases#14468 as int) AS cases#16480, cast(deaths#14469 as int) AS deaths#16482]\n",
      "                                                            +- PhotonRowToColumnar\n",
      "                                                               +- FileScan csv [county#14465,state#14466,cases#14468,deaths#14469] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[dbfs:/databricks-datasets/COVID/covid-19-data], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<county:string,state:string,cases:string,deaths:string>\n",
      "\n",
      "\n",
      "== Photon Explanation ==\n",
      "The query is fully supported by Photon.\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# VERSION 2B: LAZY/OPTIMIZED SQL QUERY ✅\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"VERSION 2B: LAZY/OPTIMIZED APPROACH\")\n",
    "print(\"-\" * 80)\n",
    "print(\"Optimizations:\")\n",
    "print(\"  ✅ Single GROUP BY with all aggregations together\")\n",
    "print(\"  ✅ One pass over data\")\n",
    "print(\"  ✅ No unnecessary joins\")\n",
    "print(\"  ✅ Efficient execution plan\")\n",
    "\n",
    "lazy_q2_start = time.time()\n",
    "\n",
    "# OPTIMIZED: Single GROUP BY with all aggregations\n",
    "sql_lazy_q2 = \"\"\"\n",
    "    SELECT \n",
    "        state,\n",
    "        COUNT(DISTINCT county) as num_counties,\n",
    "        SUM(total_cases) as state_total_cases,\n",
    "        SUM(total_deaths) as state_total_deaths,\n",
    "        ROUND(AVG(avg_mortality_rate), 2) as avg_mortality_rate,\n",
    "        MAX(total_cases) as max_county_cases\n",
    "    FROM state_stats_lazy\n",
    "    GROUP BY state\n",
    "    ORDER BY state_total_cases DESC\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n[Executing optimized query...]\")\n",
    "lazy_result_q2 = spark.sql(sql_lazy_q2)\n",
    "lazy_count_q2 = lazy_result_q2.count()\n",
    "lazy_q2_time = time.time() - lazy_q2_start\n",
    "\n",
    "print(f\"⏱️  Lazy Query 2 Time: {lazy_q2_time:.2f} seconds\")\n",
    "\n",
    "print(\"\\nExecution Plan (Optimized - notice single scan):\")\n",
    "lazy_result_q2.explain(mode=\"simple\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ac98bd9-5fe4-477b-8925-b730605c907d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PERFORMANCE COMPARISON\n",
      "================================================================================\n",
      "Eager/Inefficient Approach:   0.80 seconds\n",
      "Lazy/Optimized Approach:   0.73 seconds\n",
      "Time Saved:   0.07 seconds\n",
      "Speedup:   1.10x faster\n",
      "Performance Improvement:    8.8%\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# PERFORMANCE COMPARISON\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PERFORMANCE COMPARISON\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "speedup_q2 = eager_q2_time / lazy_q2_time\n",
    "time_saved_q2 = eager_q2_time - lazy_q2_time\n",
    "percent_improvement_q2 = ((eager_q2_time - lazy_q2_time) / eager_q2_time) * 100\n",
    "\n",
    "print(f\"Eager/Inefficient Approach: {eager_q2_time:6.2f} seconds\")\n",
    "print(f\"Lazy/Optimized Approach: {lazy_q2_time:6.2f} seconds\")\n",
    "print(f\"Time Saved: {time_saved_q2:6.2f} seconds\")\n",
    "print(f\"Speedup: {speedup_q2:6.2f}x faster\")\n",
    "print(f\"Performance Improvement: {percent_improvement_q2:6.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e3f93f7-b40d-4301-b2d6-2a7d89a52013",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Write results to a destination - my VOLUME on Databricks\n",
    "output_base = \"/Volumes/pyspark_assn_aj463/table1_output_schema/table1_volume_test1/\"\n",
    "\n",
    "output_path_state_pop = f\"{output_base}/output_path_state_pop\"\n",
    "state_population.write.mode(\"overwrite\").parquet(output_path_state_pop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f76bf75-7550-4ba8-a74c-cbcbb71b9d68",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# writing all outputs\n",
    "output_path_eager_result = f\"{output_base}/eager_result\"\n",
    "eager_result.write.mode(\"overwrite\").parquet(output_path_eager_result)\n",
    "\n",
    "output_path_eager_result = f\"{output_base}/eager_result\"\n",
    "state_population.write.mode(\"overwrite\").parquet(output_path_eager_result)\n",
    "\n",
    "output_path_lazy_result = f\"{output_base}/lazy_result\"\n",
    "lazy_result.write.mode(\"overwrite\").parquet(output_path_lazy_result)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "pyspark_assn_aj463",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
